% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sql_funs.R
\name{sql_upload}
\alias{sql_upload}
\title{Upload tables to SQL in chunks}
\usage{
sql_upload(
  data,
  conn,
  sql_table_name,
  n_chunks = NULL,
  overwrite = TRUE,
  append = FALSE
)
}
\arguments{
\item{data}{A data frame to be uploaded.}

\item{conn}{A DBI connection object, created by \code{DBI::dbConnect()}.}

\item{sql_table_name}{The name of the table in SQL to write to.}

\item{n_chunks}{The number of chunks to divide the input data into. If
\code{NULL}, no chunking will be used.}

\item{overwrite}{Whether or not to overwrite the existing data in SQL. Must
be the opposite of \code{append}.}

\item{append}{Whether or not to append data to the existing data in SQL. Must
be the opposite of \code{overwrite}.}
}
\description{
Writing large tables to a database often fails on an
intermittent connection or when the database has large active queries. This
function is a hacky workaround to upload large tables in chunks, rather than
all at once. It divides the input data (to be uploaded) into N equal-sized
chunks, then uploads them one by one.
}
\examples{
\dontrun{
# Create a SQLite database to test upload
conn <- DBI::dbConnect(RSQLite::SQLite(), ":memory:")

# Upload the iris dataset in chunks
sql_upload(
  data = iris,
  conn = conn,
  sql_table_name = "test",
  n_chunks = 5,
  overwrite = TRUE,
  append = FALSE
)
}

}
\concept{sql_funs}
